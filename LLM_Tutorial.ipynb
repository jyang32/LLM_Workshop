{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f4818c",
   "metadata": {},
   "source": [
    "# Introduction to Generative LLMs for Social Science Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc6673",
   "metadata": {},
   "source": [
    "June Yang, CSDE and eScience Institute, Winter 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53f906",
   "metadata": {},
   "source": [
    "Hello! Welcome to the workshop on using LLMs for social science research.\n",
    "\n",
    "For today's session, we will first go over the use cases of LLMs in social science research, and then dive deeper into two methods (chat completion and RAG) using the `langchain` package and an OpenAI model (gpt-3.5-turbo). As this field is quickly evolving, I encourage you to explore other models, especially the open-source ones, and think of your own research applications creatively! \n",
    "\n",
    "Proficiency in Python is not required, as coding is not the focus in this workshop, but basic understandings of writing functions and loops will be helpful. We will also be covering some relatively advanced topics in text analysis, so some prime knowledge in Text as Data will also be helpful. For a refresher, feel free to check out my previous tutorial on [Text as Data](https://colab.research.google.com/drive/1RWUytojPxMkMQs2pDo7EYKwMBRP76IUo?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0057e1",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Resources and Materials](#resources-and-materials)\n",
    "- [What is a generative LLM?](#what-is-a-generative-llm)\n",
    "- [The use of generative LLMs for the social sciences](#the-use-of-generative-llms-for-the-social-sciences)\n",
    "- [Implementation using `LangChain`](#implementation-using-langchain)\n",
    "  - [Introducting the `LangChain` package](#introducing-the-langchain-package)\n",
    "  - [Case 1: Using Chat Completion for Text Annotation](#using-chat-completion-for-text-annotation)\n",
    "    - [Models (LLM Wrappers)](#step-1-models-llm-wrappers)\n",
    "    - [Prompts](#step-2-prompts)\n",
    "    - [Chains](#step-3-chains)\n",
    "    - [Validation](#step-4-validation)\n",
    "    - [Model Selection](#model-selection)\n",
    "  - [Case 2: Embeddings, RAG, and Vector Stores](#advanced-topic-embeddings-rag-and-vector-stores)\n",
    "    - [What is word embedding](#what-is-word-embedding)\n",
    "    - [What is RAG?](#what-is-rag)\n",
    "      - [Differences between RAG and Fine-Tuning](#differences-between-rag-and-fine-tuning)\n",
    "      - [How to implement RAG?](#how-to-implement-rag)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7495a4",
   "metadata": {},
   "source": [
    "# Resources and Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da975c14",
   "metadata": {},
   "source": [
    "- [DeepSeek](https://www.deepseek.com/en)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [LangChain Documentation](https://docs.langchain.com/docs/)\n",
    "- [Hugging Face](https://huggingface.co)\n",
    "- [Parlance Labs](https://parlance-labs.com/education/prompt_eng/berryman.html)\n",
    "- [The landscape of LLMs and their openness](https://opening-up-chatgpt.github.io)\n",
    "- [eScience SSEC tutorial](https://uw-ssec-tutorials.readthedocs.io/en/latest/SciPy2024/README.html) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05612400",
   "metadata": {},
   "source": [
    "# What is a generative LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f200e",
   "metadata": {},
   "source": [
    "Recommended reading: [BERT vs. GPT: A Tale of Two Transformers that Revolutionized NLP](https://medium.com/@prudhvithtavva/bert-vs-gpt-a-tale-of-two-transformers-that-revolutionized-nlp-11fff8e61984)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60d476",
   "metadata": {},
   "source": [
    "Generative vs. Discriminative Models\n",
    "\n",
    "- Generative Models: These models learn to generate new data instances that resemble the training data. In language models, this means they can produce text, complete sentences, and engage in open-ended tasks like translation or summarization. Examples include probablistic generative models, neural generative models such as GPT (Generative Pre-trained Transformer), as well as hybrid models (e.g. topic guided transformers).\n",
    "\t\n",
    "- Discriminative (or Classifier) Models: These models focus on making predictions or classifications based on input data. For example, they might classify sentiment, identify named entities, or categorize topics. Models like BERT (Bidirectional Encoder Representations from Transformers) are discriminative since they’re optimized for predicting specific labels given input text, such as in sentiment analysis or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100bdc83",
   "metadata": {},
   "source": [
    "# The use of generative LLMs for the social sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7d4ad",
   "metadata": {},
   "source": [
    "Reference: Large Language Models (LLMs) in Social Science Research Session I: Introduction\n",
    "\n",
    "Joshua Cova and Luuk Schmitz, MPISS, 20-06-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5aaf98",
   "metadata": {},
   "source": [
    "How can LLMs help social science research?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc4ef9",
   "metadata": {},
   "source": [
    "- Modelling human behavior computationally \n",
    "  - Testing & (potentially) running experiments: Aher, Arriaga, and Kalai (2022); Dillion et al(2023)\n",
    "  - Running surveys: Tjuatia et al.(2023)\n",
    "  - Integreating social network interactions with LLMs: Jiang and Ferrara (2023)\n",
    "\n",
    "- Simulating social relationships\n",
    "  - Simulating social networks: Interactions between articifial agents: Park et al.(2023); Wang et al.(2024)\n",
    "  - Game theoretical simulations: Akata et al.(2023)\n",
    "\n",
    "- Interacting with human agents   \n",
    "  - Chatbots for interviewing partiticipants: Chopra and Haaland (2023)\n",
    "  \n",
    "- Text annotation \n",
    "  - Zero-shot & few-shot text annotation: Törnberg (2023); Gilardi, Alizadeh, and Kubi (2023); Leek, Bischl, and Freier (2024)\n",
    "  - Synthetic data generation: Laurer (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12356b",
   "metadata": {},
   "source": [
    "LLMs can also help streamline the research pipeline more generally. \n",
    "\n",
    "- Spitballing ideas\n",
    "- Data preparation\n",
    "- Summazing texts (e.g., Interview data)\n",
    "- Creative writing\n",
    "- Conducting preliminary analyses \n",
    "  \n",
    "See also Korinek (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad08c84",
   "metadata": {},
   "source": [
    "# Implementation using `LangChain`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749bddd",
   "metadata": {},
   "source": [
    "### Introducing the `LangChain` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45a66b",
   "metadata": {},
   "source": [
    "[LangChain](https://docs.langchain.com/docs/) is an open-source Python framework that streamlines the development of applications powered by Large Language Models (LLMs). It offers a suite of tools and components that simplify the construction of LLM-centric applications, making it particularly useful for social scientists interested in leveraging generative AI for their research.\n",
    "\n",
    "Key Features of LangChain:\n",
    "\n",
    "- Prompt Templates: LangChain provides reusable templates that can be dynamically adjusted by inserting specific values, allowing for the generation of prompts based on dynamic resources. ￼\n",
    "\n",
    "- Chains: These are sequences of components or actions linked together to process inputs and produce desired outputs. For example, a chain might involve retrieving data, processing it through an LLM, and then formatting the output. ￼\n",
    "\t\n",
    "- Memory: This feature allows applications to remember previous interactions, enabling more contextually relevant responses in conversational AI applications. ￼\n",
    "\t\n",
    "- Agents: Agents can perform actions based on user input, such as querying databases or calling APIs, and then use LLMs to generate responses based on the results. ￼\n",
    "\t\n",
    "- Integration with External Data: LangChain facilitates the incorporation of external data sources, such as APIs and databases, enhancing the relevance and accuracy of generated responses. ￼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548f8dd",
   "metadata": {},
   "source": [
    "### Case 1: Using Chat Completion for Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47607b87",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to replicate Törnberg (2023): [How to use Large Language Models for Text Analysis](https://arxiv.org/abs/2307.13106) by breaking the task into processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988774a0",
   "metadata": {},
   "source": [
    "What is the paper doing?\n",
    "\n",
    "This paper introduces the *WHAT* and *HOW* on using LLMs for text analysis, specifically focusing on the use of chat completion for text annotation. It offers an example of using the OpenAI API and model gpt-4 to annotate the level of populism in political speeches. Note that this is a creative and customized measurement that can be generated based on theorecial knowledge, traditionally done by human annotators. In the example below, we will be reproducing the paper, but not entirely using the OpenAI python library. Rather, we will mainly use the `LangChain` package to implement the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373134c3",
   "metadata": {},
   "source": [
    "Before we start, there are some important concepts to understand about APIs:\n",
    "\n",
    "An Application Programming Interface (API) is like a waiter at a restaurant - it acts as an intermediary that takes requests and returns responses. APIs allow different software systems to communicate without needing to understand each other's internal workings.\n",
    "\n",
    "Key concepts of APIs:\n",
    "\n",
    "- They provide a standardized way to request services or data\n",
    "- They define the methods and data formats for interaction\n",
    "- They handle authentication and access control\n",
    "- They often have usage limits and pricing tiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e43a10",
   "metadata": {},
   "source": [
    "For LLMs like OpenAI's GPT models or Anthropic's Claude, APIs allow developers to:\n",
    "\n",
    "- Access Model Capabilities\n",
    "  - Send text prompts and receive generated responses\n",
    "  - Control various parameters like temperature (creativity) and max tokens\n",
    "  - Access different model versions and capabilities\n",
    "  \n",
    "- Manage Resources\n",
    "  - Track usage and costs\n",
    "  - Handle rate limiting and quotas\n",
    "  - Manage API keys and authentication\n",
    "  \n",
    "- Integration Options\n",
    "  - Direct API calls for basic interactions\n",
    "  - SDK (Software Development Kit) support for easier integration\n",
    "  - Framework support through tools like LangChain\n",
    "  \n",
    "- Common LLM API Features\n",
    "  - Authentication\n",
    "    - API keys for secure access\n",
    "    - Organization IDs for team management\n",
    "  - Request Parameters\n",
    "    - Model selection (e.g., GPT-4, GPT-3.5-turbo)\n",
    "    - Temperature and other generation controls\n",
    "    - System and user messages\n",
    "    - Context and memory management\n",
    "\n",
    "  - Response Handling\n",
    "    - Generated text\n",
    "    - Token usage statistics\n",
    "    - Error messages and rate limit information\n",
    "\n",
    "This API-based approach allows researchers and developers to leverage powerful LLM capabilities without needing to host or maintain the models themselves. For an example, see the [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction), [playground](https://platform.openai.com/playground), and [pricing](https://openai.com/api/pricing/) page. Of course, with the lastest development of DeepSeek, hosting your own model has become an option as well (actually, a preferable option). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb7c456",
   "metadata": {},
   "source": [
    "We'll proceed with replicating the paper in the below sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5720b",
   "metadata": {},
   "source": [
    "#### Step 1. Models (LLM Wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73fd6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages in your environment:\n",
    "# pip install langchain openai pandas scikit-learn dotenv\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain_community.llms import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "import time  \n",
    "\n",
    "# optional: suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Specifically for LangChain deprecation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f420c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Set up OpenAI API key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "# Test the LLM\n",
    "print(llm.invoke(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6023eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3d752",
   "metadata": {},
   "source": [
    "Dataset we are using: Hawkins, Kirk A., Rosario Aguilar, Erin Jenne, Bojana Kocijan, Cristóbal Rovira Kaltwasser, Bruno Castanho Silva. 2019. Global Populism Database: Populism Dataset for Leaders 1.0. [link](https://populism.byu.edu/data/2019%20-%20global%20populism%20database%20(guardian%20version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82766510",
   "metadata": {},
   "source": [
    "The political texts are in different languages, and possibly follows different text encoding standards. We define a function on auto encoding below. Note that this is also a common step we see when working with different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15a35e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_with_auto_encoding(file_path):\n",
    "    # First, detect the encoding\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            text = file.read()\n",
    "            # Remove BOM if present\n",
    "            if text.startswith('\\ufeff'):\n",
    "                text = text[1:]\n",
    "                \n",
    "            # Find the actual speech content\n",
    "            lines = text.split('\\n')\n",
    "            content_start = 0\n",
    "            \n",
    "            # First try: Look for quotation marks\n",
    "            for i, line in enumerate(lines):\n",
    "                if '\"' in line or '\"' in line:\n",
    "                    content_start = i\n",
    "                    break\n",
    "            \n",
    "            # Second try: Look for first substantial text after metadata\n",
    "            if content_start == 0:\n",
    "                empty_lines = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    if not line.strip():\n",
    "                        empty_lines += 1\n",
    "                    elif empty_lines >= 2:  # After finding 2+ empty lines, next non-empty is content\n",
    "                        content_start = i\n",
    "                        break\n",
    "            \n",
    "            # Third try: Look for first empty line (simpler fallback)\n",
    "            if content_start == 0:\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.strip() == '' and i > 0:\n",
    "                        content_start = i + 1\n",
    "                        break\n",
    "            \n",
    "            # Get the content\n",
    "            content = '\\n'.join(lines[content_start:])\n",
    "            \n",
    "            # Clean up any leading/trailing whitespace\n",
    "            content = content.strip()\n",
    "            \n",
    "            return content\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 if detection fails\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            text = file.read()\n",
    "            if text.startswith('\\ufeff'):\n",
    "                text = text[1:]\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e110a7",
   "metadata": {},
   "source": [
    "We will now read all the speeches from the folder and store them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eab075d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows:\n",
      "                              speech_name  \\\n",
      "0               Nicaragua_Ortega_Famous_1   \n",
      "1                  France_Chirac_Famous_1   \n",
      "2                   Serbia_Tadic_Famous_1   \n",
      "3  Georgia_Margvelashvili_International_1   \n",
      "4                       UK_Blair_Ribbon_3   \n",
      "\n",
      "                                                text  \n",
      "0  Primer Año del Gobierno del Pueblo\\nPrimer Ani...  \n",
      "1  \"Déclaration aux Français de Monsieur Jacques ...  \n",
      "2  Држава Србија је забринута што је изостала реа...  \n",
      "3                                                     \n",
      "4  A Cabinet Minister is sitting having a pub lun...  \n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing speech text files\n",
    "folder_path = 'data/global-populism-dataset-zi/speeches_20220427/speeches_20220427'\n",
    "# Initialize an empty list to store speech data\n",
    "data = []\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            text = read_file_with_auto_encoding(file_path)\n",
    "            speech_name = os.path.splitext(filename)[0]\n",
    "            data.append({'speech_name': speech_name, 'text': text})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read file {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Create a DataFrame with the collected data\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62ed9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of first 100 characters from each text:\n",
      "\n",
      "Nicaragua_Ortega_Famous_1:\n",
      "Primer Año del Gobierno del Pueblo\n",
      "Primer Aniversario del Gobierno de \n",
      "Reconciliación y Unidad Nacio\n",
      "\n",
      "France_Chirac_Famous_1:\n",
      "\"Déclaration aux Français de Monsieur Jacques CHIRAC, Président de la République.\n",
      "\n",
      "Palais de l'Elysé\n",
      "\n",
      "Serbia_Tadic_Famous_1:\n",
      "Држава Србија је забринута што је изостала реакција Уједињениих нација, УНМИК-а, поводом проглашења \n",
      "\n",
      "Georgia_Margvelashvili_International_1:\n",
      "\n",
      "\n",
      "UK_Blair_Ribbon_3:\n",
      "A Cabinet Minister is sitting having a pub lunch when a member of the public accosts him.  “I’ve bee\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of text from each row to verify encoding\n",
    "print(\"\\nSample of first 100 characters from each text:\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"\\n{row['speech_name']}:\")\n",
    "    print(row['text'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b4dce",
   "metadata": {},
   "source": [
    "Each model has a token limit, which is the maximum number of tokens that can be processed in a single request. We will now define a function to count the number of tokens in a text using the `tiktoken` package. When using the API request, the `gpt-3.5-turbo` model we are using is the 16k variant, with a token limit of 16384. This includes the system prompt, user prompt, and response. For details, see OpenAI's documentation [here](https://platform.openai.com/docs/models#gpt-3-5-turbo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "819c9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              speech_name  \\\n",
      "0               Nicaragua_Ortega_Famous_1   \n",
      "1                  France_Chirac_Famous_1   \n",
      "2                   Serbia_Tadic_Famous_1   \n",
      "3  Georgia_Margvelashvili_International_1   \n",
      "4                       UK_Blair_Ribbon_3   \n",
      "\n",
      "                                              chunks  \n",
      "0  [Primer Año del Gobierno del Pueblo\\nPrimer An...  \n",
      "1  [\"Déclaration aux Français de Monsieur Jacques...  \n",
      "2  [Држава Србија је забринута што је изостала ре...  \n",
      "3                                                 []  \n",
      "4  [A Cabinet Minister is sitting having a pub lu...  \n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Initialize the tokenizer for GPT-3.5-turbo\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,  # Approximate number of tokens\n",
    "    chunk_overlap=200,  # Some overlap to maintain context\n",
    "    length_function=lambda text: len(encoding.encode(text)),  # Use the same token counter\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Default separators\n",
    ")\n",
    "\n",
    "# Apply the splitter to each text in the DataFrame\n",
    "df['chunks'] = df['text'].apply(lambda x: text_splitter.split_text(x))\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df[['speech_name', 'chunks']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0e6e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count distribution:\n",
      "chunk_count\n",
      "0    111\n",
      "1    943\n",
      "2     71\n",
      "3     22\n",
      "4      7\n",
      "5      5\n",
      "6      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example rows with chunk counts:\n",
      "                              speech_name  chunk_count\n",
      "0               Nicaragua_Ortega_Famous_1            2\n",
      "1                  France_Chirac_Famous_1            1\n",
      "2                   Serbia_Tadic_Famous_1            1\n",
      "3  Georgia_Margvelashvili_International_1            0\n",
      "4                       UK_Blair_Ribbon_3            1\n",
      "\n",
      "Summary statistics of chunk counts:\n",
      "count    1161.000000\n",
      "mean        1.047373\n",
      "std         0.629993\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         6.000000\n",
      "Name: chunk_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get length of chunks list for each row\n",
    "df['chunk_count'] = df['chunks'].apply(len)\n",
    "\n",
    "# Show distribution of chunk counts\n",
    "print(\"Chunk count distribution:\")\n",
    "print(df['chunk_count'].value_counts().sort_index())\n",
    "\n",
    "# Show some example rows with their chunk counts\n",
    "print(\"\\nExample rows with chunk counts:\")\n",
    "print(df[['speech_name', 'chunk_count']].head())\n",
    "\n",
    "# Get summary statistics\n",
    "print(\"\\nSummary statistics of chunk counts:\")\n",
    "print(df['chunk_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8488a1",
   "metadata": {},
   "source": [
    "Remove the zero-chunk rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "703f94bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of speeches: 1161\n",
      "Number after removing empty chunks: 1050\n",
      "Removed 111 speeches\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where chunks list is empty\n",
    "df_clean = df[df['chunks'].apply(len) > 0].copy()\n",
    "\n",
    "# Print before/after stats\n",
    "print(f\"Original number of speeches: {len(df)}\")\n",
    "print(f\"Number after removing empty chunks: {len(df_clean)}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} speeches\")\n",
    "\n",
    "# Reset the index if needed\n",
    "df_clean = df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e27aa15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   speech_name  1050 non-null   object\n",
      " 1   text         1050 non-null   object\n",
      " 2   chunks       1050 non-null   object\n",
      " 3   chunk_count  1050 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6556ba7",
   "metadata": {},
   "source": [
    "#### Step 2. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2431ba0",
   "metadata": {},
   "source": [
    "Writing the prompt is a crucial step in using LLMs. The prompt is the input to the LLM, and it is the only way to control the output of the LLM. In this step, we will be writing the prompt for the chat completion task. This process is often iterative, relying on the output of the LLM to refine the prompt. The iterative nature of the prompting process gives it the name **prompt engineering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3dc60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "# Define the base prompt template\n",
    "system_prompt = \"\"\"You are an expert in analyzing political speeches for populist content. \n",
    "You can analyze text in any language.\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"Your task is to evaluate the level of populism in this political text:\n",
    "\n",
    "{text}\n",
    "\n",
    "A populist text must contain BOTH of these elements:\n",
    "\n",
    "1. People-centrism:\n",
    "- Focus on \"the people\" or \"ordinary people\" as an indivisible/homogeneous community\n",
    "- Promotes politics as the popular will of \"the people\"\n",
    "- NOTE: Appeals to specific subgroups (ethnicities, regional groups, classes) are NOT populist\n",
    "\n",
    "2. Anti-elitism:\n",
    "- Focus on \"the elite\" with negative descriptions\n",
    "- Presents elite vs people as a moral struggle between good and bad\n",
    "- NOTE: Criticism of specific elite members is NOT populist - must reject elite as a whole\n",
    "\n",
    "Rate from 0-2:\n",
    "0 = Not populist\n",
    "1 = Somewhat populist\n",
    "2 = Highly populist\n",
    "\n",
    "Respond with: [score]; [brief justification]\"\"\"\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", human_prompt),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6ec71",
   "metadata": {},
   "source": [
    "### Step 3. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9e46d",
   "metadata": {},
   "source": [
    "What is a chain?\n",
    "\n",
    "A chain is a sequence of steps that are executed in order. In the context of LangChain, a chain is a sequence of components or actions linked together to process inputs and produce desired outputs. For example, a chain might involve retrieving data, processing it through an LLM, and then formatting the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ef4b1",
   "metadata": {},
   "source": [
    "Let's first test the prompt with a single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43d75c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk = df_clean.iloc[0]['chunks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4657451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input chunk preview:\n",
      "Primer Año del Gobierno del Pueblo\n",
      "Primer Aniversario del Gobierno de \n",
      "Reconciliación y Unidad Nacional, \n",
      "Gobierno del Poder Ciudadano \n",
      "Plaza de la Revolución\n",
      "\n",
      "10 de enero del 2007 \n",
      "\n",
      "\n",
      "“Quiero, ante ustedes, citar nuevamente al Papa Benedicto XVI. Juan Pablo decía: capitalismo salvaje, los pueblos no ...\n",
      "\n",
      "LLM Response:\n",
      "Score: 2\n",
      "Justification: The text contains strong elements of people-centrism by repeatedly referring to \"el pueblo\" and emphasizing the importance of the people's power and participation in decision-making. It also includes anti-elitism by criticizing the capitalist system and highlighting the need to transform global development models that benefit a minority elite. The speech frames the struggle as one between the people and the elite, portraying the elite as responsible for societal injustices.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.schema.output_parser import StrOutputParser  \n",
    "\n",
    "# Create the chain\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Test prompt with single chunk\n",
    "try:\n",
    "    # Use the chain we created earlier\n",
    "    response = chain.invoke({\"text\": test_chunk})\n",
    "    \n",
    "    print(\"Input chunk preview:\")\n",
    "    print(test_chunk[:300], \"...\\n\")\n",
    "    \n",
    "    print(\"LLM Response:\")\n",
    "    score, justification = response.split(';', 1)\n",
    "    print(f\"Score: {score.strip()}\")\n",
    "    print(f\"Justification: {justification.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing text: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816a7ab",
   "metadata": {},
   "source": [
    "Now we apply it to all the speeches in the dataframe if we are satisfied with the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad2648b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in Speech: Australia_Morrison_Ribbon_1\n",
      "Chunk 0\n",
      "Response causing error: [score]; The speech by the Prime Minister of Australia does not exhibit clear elements of populism. While it may contain some elements of people-centrism by honoring the sacrifices of Australian soldiers, it does not display anti-elitism or frame politics as a struggle between the elite and the people.\n",
      "Error type: ValueError\n",
      "\n",
      "Error in Speech: Colombia_Santos_International_1\n",
      "Chunk 0\n",
      "Response causing error: [score]; [brief justification]\n",
      "Error type: ValueError\n"
     ]
    }
   ],
   "source": [
    "# Create the chain\n",
    "# chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Process all rows in the dataframe\n",
    "results = []\n",
    "\n",
    "for index, row in df_clean.iterrows():\n",
    "    chunk_scores = []\n",
    "    chunk_justifications = []\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(row['chunks']):\n",
    "        try:\n",
    "            response = chain.invoke({\"text\": chunk})\n",
    "            \n",
    "            if ';' in response:\n",
    "                score_part, justification = response.split(';', 1)\n",
    "                score_str = score_part.replace('Score:', '').strip()\n",
    "                score = float(score_str)\n",
    "                chunk_scores.append(score)\n",
    "                chunk_justifications.append(justification.strip())\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Only print when there's an error\n",
    "            print(f\"\\nError in Speech: {row['speech_name']}\")\n",
    "            print(f\"Chunk {chunk_idx}\")\n",
    "            print(f\"Response causing error: {response}\")\n",
    "            print(f\"Error type: {type(e).__name__}\")\n",
    "            continue\n",
    "    \n",
    "    if chunk_scores:\n",
    "        avg_score = sum(chunk_scores) / len(chunk_scores)\n",
    "        results.append({\n",
    "            'speech_name': row['speech_name'],\n",
    "            'populism_score': round(avg_score, 2),\n",
    "            'num_chunks_processed': len(chunk_scores),\n",
    "            'justifications': chunk_justifications\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3691105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1044 speeches\n",
      "\n",
      "First few results:\n",
      "                        speech_name  populism_score  num_chunks_processed  \\\n",
      "0         Nicaragua_Ortega_Famous_1             2.0                     2   \n",
      "1            France_Chirac_Famous_1             2.0                     1   \n",
      "2             Serbia_Tadic_Famous_1             2.0                     1   \n",
      "3                 UK_Blair_Ribbon_3             2.0                     1   \n",
      "4  Malaysia_Mohamad_International_1             1.0                     1   \n",
      "\n",
      "                                      justifications  \n",
      "0  [The text focuses heavily on \"the people\" and ...  \n",
      "1  [This text contains strong elements of populis...  \n",
      "2  [The text exhibits a high level of populism as...  \n",
      "3  [This political text contains a strong emphasi...  \n",
      "4  [The speech does contain elements of populism,...  \n"
     ]
    }
   ],
   "source": [
    "# Create final results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Processed {len(results_df)} speeches\")\n",
    "print(\"\\nFirst few results:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b1368e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "results_df.to_csv('data/populism_analysis_results4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dac633",
   "metadata": {},
   "source": [
    "#### Step 4. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787526d",
   "metadata": {},
   "source": [
    "According to Törnberg (2023): \n",
    "\n",
    "\"The Krippendorff’s alpha gives a measure of interrater agreement, and is used to assess the extent to which multiple raters or coders agree when coding or categorizing qualitative data. Krippendorff’s alpha takes into account both the observed agreement among raters and the expected agreement by chance. It can be applied to different types of nominal, ordinal, or interval-level data.\"\n",
    "\n",
    "Hypothetically, if we have the 'ground truth' data provided by human annotators, we are able to use the Krippendorff's alpha for reliability check (illustration purpose only in the example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d603c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches in original df: 1050\n",
      "Number of speeches in results_df: 1044\n",
      "Number of matched speeches: 1044\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from krippendorff import alpha\n",
    "\n",
    "# First, create a mapping of speech names to their indices in the original df\n",
    "original_speech_map = {name: idx for idx, name in enumerate(df['speech_name'])}\n",
    "\n",
    "# Create random ground truth scores (0, 1, or 2) for the original dataframe\n",
    "np.random.seed(42)  # for reproducibility\n",
    "df['ground_truth'] = np.random.choice([0, 1, 2], size=len(df))\n",
    "\n",
    "# Create a merged dataframe that includes both LLM scores and ground truth\n",
    "merged_df = pd.merge(\n",
    "    results_df,\n",
    "    df[['speech_name', 'ground_truth']],\n",
    "    on='speech_name',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Number of speeches in original df:\", len(df_clean))\n",
    "print(\"Number of speeches in results_df:\", len(results_df))\n",
    "print(\"Number of matched speeches:\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b6c1b",
   "metadata": {},
   "source": [
    "Out of curiosity, let's see if there are any speeches that weren't processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c22ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of missing annotations:\n",
      "Total speeches in original dataset: 1161\n",
      "Speeches processed by model: 1044\n",
      "Speeches missing annotations: 117\n",
      "\n",
      "Sample of unprocessed speeches:\n",
      "                               speech_name chunks\n",
      "3   Georgia_Margvelashvili_International_1     []\n",
      "15          Kazakhstan_Nazarbayev_Ribbon_2     []\n",
      "24           Estonia_Ansip_International_3     []\n",
      "27           Estonia_Ansip_International_2     []\n",
      "37        Bulgaria_Borisov_International_1     []\n"
     ]
    }
   ],
   "source": [
    "# Find speeches that weren't processed by the model\n",
    "unprocessed_df = df[~df['speech_name'].isin(results_df['speech_name'])]\n",
    "\n",
    "print(\"Analysis of missing annotations:\")\n",
    "print(f\"Total speeches in original dataset: {len(df)}\")\n",
    "print(f\"Speeches processed by model: {len(results_df)}\")\n",
    "print(f\"Speeches missing annotations: {len(unprocessed_df)}\")\n",
    "\n",
    "# Display some details about the unprocessed speeches\n",
    "if len(unprocessed_df) > 0:\n",
    "    print(\"\\nSample of unprocessed speeches:\")\n",
    "    print(unprocessed_df[['speech_name', 'chunks']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d23e6",
   "metadata": {},
   "source": [
    "Why are these speeches not processed by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f7ebaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of speeches with empty chunks:\n",
      "\n",
      "Georgia_Margvelashvili_International_1:\n",
      "First 200 characters: \n",
      "Text length: 0\n",
      "Is text empty? True\n",
      "\n",
      "Kazakhstan_Nazarbayev_Ribbon_2:\n",
      "First 200 characters: \n",
      "Text length: 0\n",
      "Is text empty? True\n",
      "\n",
      "Estonia_Ansip_International_3:\n",
      "First 200 characters: \n",
      "Text length: 0\n",
      "Is text empty? True\n",
      "\n",
      "Estonia_Ansip_International_2:\n",
      "First 200 characters: \n",
      "Text length: 0\n",
      "Is text empty? True\n",
      "\n",
      "Bulgaria_Borisov_International_1:\n",
      "First 200 characters: \n",
      "Text length: 0\n",
      "Is text empty? True\n"
     ]
    }
   ],
   "source": [
    "# Examine the original text of speeches with empty chunks\n",
    "print(\"Sample of speeches with empty chunks:\")\n",
    "for idx, row in unprocessed_df.head().iterrows():\n",
    "    speech_name = row['speech_name']\n",
    "    original_text = df[df['speech_name'] == speech_name]['text'].values[0]\n",
    "    print(f\"\\n{speech_name}:\")\n",
    "    print(f\"First 200 characters: {original_text[:200]}\")\n",
    "    print(f\"Text length: {len(original_text)}\")\n",
    "    print(f\"Is text empty? {len(original_text.strip()) == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a0345",
   "metadata": {},
   "source": [
    "The empty text points to the need of possibly improving the auto-encoding function as well as how we cleaned empty chunks in above steps (note the possible difference between an empty chunk list and an empty chunk).\n",
    "\n",
    "In the below code, we will calculate the Krippendorff's alpha to check the reliability of the model (illustration purpose only in our example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67894b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Krippendorff's alpha: 0.14489115221108617\n",
      "\n",
      "Agreement Statistics:\n",
      "Exact matches: 31.23%\n",
      "Average difference: 0.90\n",
      "\n",
      "Score Distribution:\n",
      "LLM Scores:\n",
      "populism_score\n",
      "0.00     27\n",
      "0.75      1\n",
      "1.00    358\n",
      "1.25      1\n",
      "1.33      2\n",
      "1.40      2\n",
      "1.50     23\n",
      "1.60      2\n",
      "1.67      7\n",
      "1.75      2\n",
      "1.83      1\n",
      "2.00    618\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ground Truth:\n",
      "ground_truth\n",
      "0    370\n",
      "1    337\n",
      "2    337\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Krippendorff's alpha\n",
    "# Convert to matrix where each row is a speech and each column is a rater\n",
    "reliability_data = np.array([\n",
    "    merged_df['populism_score'].values,\n",
    "    merged_df['ground_truth'].values\n",
    "])\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "k_alpha = alpha(reliability_data=reliability_data.T, level_of_measurement='ordinal')\n",
    "\n",
    "print(\"\\nKrippendorff's alpha:\", k_alpha)\n",
    "\n",
    "# Basic agreement statistics\n",
    "agreement_df = merged_df.copy()\n",
    "agreement_df['exact_match'] = agreement_df['populism_score'] == agreement_df['ground_truth']\n",
    "agreement_df['diff'] = abs(agreement_df['populism_score'] - agreement_df['ground_truth'])\n",
    "\n",
    "print(\"\\nAgreement Statistics:\")\n",
    "print(f\"Exact matches: {agreement_df['exact_match'].mean():.2%}\")\n",
    "print(f\"Average difference: {agreement_df['diff'].mean():.2f}\")\n",
    "\n",
    "# Distribution of scores\n",
    "print(\"\\nScore Distribution:\")\n",
    "print(\"LLM Scores:\")\n",
    "print(merged_df['populism_score'].value_counts().sort_index())\n",
    "print(\"\\nGround Truth:\")\n",
    "print(merged_df['ground_truth'].value_counts().sort_index())\n",
    "\n",
    "# Save the merged results\n",
    "merged_df.to_csv('data/populism_analysis_with_ground_truth4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bdc12",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc196ae7",
   "metadata": {},
   "source": [
    "Cova and Schmitz (2024) recommend two studies on LLM model selection:\n",
    "\n",
    "Törnberg's six principles for model selection:\n",
    "\n",
    "- Reproducibility\n",
    "- Ethnic & legality\n",
    "- Transparency\n",
    "- Culture and Language\n",
    "- Scalability\n",
    "- Complexity\n",
    "\n",
    "The teacher-student model proposed by Weber and Reichardt (2024):\n",
    "\n",
    "<img src=\"image.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebff811",
   "metadata": {},
   "source": [
    "### Case 2: Embeddings, RAG, and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0940d4",
   "metadata": {},
   "source": [
    "#### What is word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242c37b",
   "metadata": {},
   "source": [
    "In the previous workshop, we learned about text representations where every document is represented by a (weighted) sum of the words it contains. In these models, the reprentation of a word is a **one-hot encoding** - a vector with one dimension per unique word in the vocabulary containing a 1 if the word is present in the document and 0 otherwise.\n",
    "\n",
    "Example: we represent a word, *cat*, using the one-hot encoding reprentation with a vector of length the size of the vocabulary *J*, where the dimension corresponding to cat is 1 and all other dimensions are 0.\n",
    "\n",
    "cat = (0,0,0,1,0,0,...0)\n",
    "\n",
    "A consequence of this representation is that every word is treated uniquely, and the similarity between words is not captured. For example, *cat* and *dog* are completely different words, and thus represented by completely different vectors.\n",
    "\n",
    "However, in reality we know that many words have highly similar meanings. The distributed representations covered in this chapter generalize this idea by learning from external data the semantic relationships between words like *writer* and *author* even though they don't share a common stem. \n",
    "\n",
    "With the above example, we will instead represent the word *cat* by using data to estimate a dense vector of length *K*, where *K < J*:\n",
    "\n",
    "cat = (1.3, 0.2,...,0.56).\n",
    "\n",
    "Because these techniques embed words into a common low-dimensional space (relative to the size of the vocabulary), they are called **word embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708f4e2",
   "metadata": {},
   "source": [
    "#### What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357d6bc",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is a cutting-edge approach in natural language processing that combines document retrieval and generative modeling to create contextually aware and accurate responses. It relies heavily on word embeddings to connect these two steps: retrieval and generation.\n",
    "\n",
    "**Retrieval with Word Embeddings**\n",
    "\n",
    "The retrieval phase identifies relevant documents or knowledge snippets from a large external database or corpus. Here’s how word embeddings come into play:\n",
    "\n",
    "- Encoding Query and Documents: The input query and all documents in the corpus are converted into vector representations using word embedding techniques (e.g., sentence embeddings, trained using models like BERT or Sentence Transformers). These embeddings map textual data into a shared high-dimensional semantic space.\n",
    "\t\n",
    "- Similarity Search: The query embedding is compared against the document embeddings using similarity metrics such as cosine similarity. This enables RAG to retrieve documents not only based on exact keyword matches but also on semantic meaning. For example, a query about “climate change effects” might retrieve documents discussing “global warming impacts” due to their semantic proximity.\n",
    "\n",
    "**Generation with Retrieved Context**\n",
    "\n",
    "Once the most relevant documents are retrieved, they serve as additional context for the generative model. This process unfolds as follows:\n",
    "\n",
    "- Incorporating Context: The retrieved documents are concatenated with the input query or transformed into prompts for the generative model. This gives the model access to external, up-to-date, or domain-specific information that complements its pre-trained knowledge.\n",
    "\t\n",
    "- Text Generation: Using the combined input, the generative model produces a coherent, informed, and contextually relevant response. For instance, if the retrieved document explains “reducing stress through mindfulness,” the model might generate an answer to a query about meditation benefits that incorporates this information.\n",
    "\n",
    "Example:\n",
    "\n",
    "If a user asks, “What are the benefits of renewable energy?”:\n",
    "\n",
    "- The retrieval phase uses embeddings to locate documents discussing renewable energy and its impacts, such as reducing carbon emissions or promoting sustainability.\n",
    "\n",
    "- The generation phase takes this retrieved data and produces a response like: “Renewable energy offers numerous benefits, including reducing greenhouse gas emissions, decreasing reliance on fossil fuels, and supporting environmental sustainability.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c732b61",
   "metadata": {},
   "source": [
    "#### Differences between RAG and Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13521b",
   "metadata": {},
   "source": [
    "RAG and fine-tuning are two distinct methods for enhancing the performance of generative models in natural language processing, each with its own approach to leveraging knowledge and improving accuracy.\n",
    "\n",
    "**Fine-Tune**\n",
    "\n",
    "Fine-tuning involves adapting a pre-trained model to a specific task or domain by updating its weights using labeled training data. This process essentially embeds task-specific knowledge directly into the model. Here’s how it works:\n",
    "\n",
    "- Data Requirements: Requires a dataset of input-output pairs relevant to the target task or domain (e.g., medical summaries, legal text generation).\n",
    "\n",
    "- Training Process: The model is trained on this data by adjusting its internal parameters, essentially “learning” the patterns and specific nuances of the target task.\n",
    "\n",
    "- Output: After fine-tuning, the model is self-contained, capable of generating context-aware responses without needing access to external knowledge.\n",
    "\n",
    "Limitations: Fine-tuning is resource-intensive, requiring large amounts of labeled data and computational power. It also “locks in” knowledge from the training set, making it difficult to update the model with new information.\n",
    "\n",
    "**RAG**\n",
    "\n",
    "RAG, in contrast, bypasses the need to encode task-specific knowledge into the model itself. Instead, it integrates a retrieval step to dynamically fetch relevant information from an external database or knowledge corpus at runtime. Here’s how it differs:\n",
    "\n",
    "- Data Requirements: Relies on a knowledge base (e.g., a document corpus, Wikipedia) rather than labeled fine-tuning data.\n",
    "\n",
    "- Process: explained above.\n",
    "    \n",
    "- Output: RAG’s responses are informed by up-to-date or domain-specific knowledge without altering the underlying model.\n",
    "\n",
    "Advantages: RAG is flexible and scalable, allowing the knowledge base to be updated independently of the model. It is also less resource-intensive as it doesn’t require retraining the model for every new task or domain.\n",
    "\n",
    "Example:\n",
    "\n",
    "For a task like answering questions about current events:\n",
    "\n",
    "- Fine-Tuning: The model would need to be retrained with labeled examples of questions and answers about current events.\n",
    "\t\n",
    "- RAG: The model dynamically retrieves recent articles about the event and generates a response using that information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee220ef",
   "metadata": {},
   "source": [
    "#### How to implement RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ddc8e",
   "metadata": {},
   "source": [
    "In this example below, we will be using the `LangChain` library and the same model, gpt-3.5-turbo, to implement RAG. This section is adapted from the example introduced in the following video: [Building a RAG application from scratch using Python, LangChain, and the OpenAI API](https://www.youtube.com/watch?v=BrsocJb-fAo). We will be using the materials from this tutorial, a transcript of an interivew, to build a RAG application that can answer questions based on the content of the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00ec74",
   "metadata": {},
   "source": [
    "#### Set up the environment and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57b1516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI technologies are used to develop systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI is a rapidly evolving field with applications in various industries, including healthcare, finance, transportation, and entertainment.\n"
     ]
    }
   ],
   "source": [
    "# We've done this step in the previous section.\n",
    "# Set up OpenAI API key and Pinecone API key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "# Test the LLM\n",
    "print(llm.invoke(\"What is Artificial Intelligence?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b916d",
   "metadata": {},
   "source": [
    "##### Set up the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6075bf",
   "metadata": {},
   "source": [
    "Note that in the below template, the context is the transcript of the interview, and the question is the user's query. These are the two input variables that we will be using to build our RAG application. We generate the prompt template first (without talking to the model at this stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a9884a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply with \"I don\\'t know.\"\\n\\nContext: Mary\\'s sister is Susana.\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply with \"I don't know.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana.\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c499d63",
   "metadata": {},
   "source": [
    "We then chain the prompt with our model and an output parser, `StrOutputParser()`, to parse the output into a string. Chain is a sequence of steps that are executed in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6a41491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Susana'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana.\", \n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b1b33",
   "metadata": {},
   "source": [
    "##### Reading in the transcript as context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911487e",
   "metadata": {},
   "source": [
    "OpenAI's [whisper](https://github.com/openai/whisper) model is a speech-to-text model that can be used to transcribe audio files. Here, we will directly read in the transcript of the interview as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65a69e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea172b",
   "metadata": {},
   "source": [
    "Can we use the entire transcript as the context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8477809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 47050 tokens. Please reduce the length of the messages.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is reading papers a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162f901",
   "metadata": {},
   "source": [
    "Again, we will be splitting the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35e19573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the transcript in memory\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/transcription.txt\")\n",
    "text_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e835868",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30593b91",
   "metadata": {},
   "source": [
    "Here is how embeddings come into play. When we pass the question, an embedding model encodes the question and the chunked context to find the most relevant chunks by calculating the similarity between the question and the chunks. We can then select the chunks with the highest similarity to the question, and use them as the context for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe5417",
   "metadata": {},
   "source": [
    "Let's play it out with the short sentences example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fe3e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[-0.0013799423062174738, -0.034500904821185445, -0.011502386217451244, 0.0012415571995061765, -0.026119610627742672, 0.009081818279194359, -0.01564924997673654, 0.001727859744481776, -0.011827630141686142, -0.03319992539895551]\n"
     ]
    }
   ],
   "source": [
    "# Note: here I am using an old version of OpenAI (0.28.1) because of my python version\n",
    "# Syntax might be different for openai version 1.0.0 and above.\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8bba7",
   "metadata": {},
   "source": [
    "We can now generate the embeddings of the below sentences, and calculate the cosine similarity between the query and the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd46969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
    "sentence2 = embeddings.embed_query(\"Pedro's mother is a teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62258ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9173235346162653, 0.7679756802174776)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab1311",
   "metadata": {},
   "source": [
    "##### What is a Vector Store?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc631f4e",
   "metadata": {},
   "source": [
    "A vectorstore is a specialized data storage system designed to store, index, and retrieve vectors efficiently. In the context of machine learning and NLP, vectors are numerical representations of data, such as word embeddings, document embeddings, or feature representations of any data point. Vectorstores are especially useful for similarity search, clustering, and other operations where comparisons of high-dimensional data are required.\n",
    "\n",
    "To put it simply, a vectorstore is a database of embeddings, and specifically designed for similarity search on the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "113a5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "514abdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"Mary's sister is Susana\"), 0.9173235428839119),\n",
       " (Document(page_content='Mary has two siblings'), 0.9045029978848249),\n",
       " (Document(page_content='John and Tommy are brothers'), 0.8013182122337668)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452349b9",
   "metadata": {},
   "source": [
    "After understanding the example above, we can now connect the vector store with a chain. To achieve this, we need to configure a [retriever](https://python.langchain.com/docs/how_to/#retrievers) in addition to the chain we created in the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff64df08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Mary's sister is Susana\"),\n",
       " Document(page_content='Mary has two siblings'),\n",
       " Document(page_content='John and Tommy are brothers'),\n",
       " Document(page_content=\"Pedro's mother is a teacher\")]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0393aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Patricia likes white cars'),\n",
       "  Document(page_content='Lucia drives an Audi'),\n",
       "  Document(page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(page_content=\"Mary's sister is Susana\")],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e1bd6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | llm | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f7d1d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audi'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | llm | parser\n",
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d5ecc",
   "metadata": {},
   "source": [
    "Now let's create a new vector store with the chunks of the transcript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae2c9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cef9a7",
   "metadata": {},
   "source": [
    "Let's set up a new chain using the new vector store. This time we are using a different equivalent syntax to specify the RunnableParallel portion of the chain. We then use `chain.invoke()` to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "245450d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development, potentially uncovering and solving the puzzle of the universe. It is suggested that synthetic AIs will be able to generate art, ideas, and emotions in an automated way.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb741c6",
   "metadata": {},
   "source": [
    "##### Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaa25b",
   "metadata": {},
   "source": [
    "How do we evaluate the performance of LLMs on a RAG task?\n",
    "\n",
    "Evaluating an LLM on RAG requires measuring both the retrieval effectiveness and genration quality. In the process of evaluating retrieval effectiveness, it is recommended to use multiple retrieval metrics (e.g., precision, recall, F1 scores). For an evaluation of the generation quality, we can look into factual correctness, relevance, and coherence of the generated response. A combination of automated approaches and human evaluation is always recommended.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699e304",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9667885",
   "metadata": {},
   "source": [
    "- Modeling Human Behavior Computationally\n",
    "\t- Aher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai. (2022). Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. [arXiv](https://doi.org/10.48550/ARXIV.2208.10264).\n",
    "\t-\tArgyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. (2023). Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis, 31(3), 337–51. [link](https://doi.org/10.1017/pan.2023.2).\n",
    "\t-\tDillion, Danica, Niket Tandon, Yuling Gu, and Kurt Gray. (2023). Can AI Language Models Replace Human Participants? Trends in Cognitive Sciences, 27(7), 597–600. [link](https://doi.org/10.1016/j.tics.2023.04.008).\n",
    "    -\tJiang, Ferrara. (2023). Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data. [arXiv](https://arxiv.org/abs/2401.00893?utm_source=chatgpt.com)\n",
    "\t-   Tjuatja, Lindia, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig.“Do LLMs Exhibit Human-Like Response Biases? A Case Study in Survey Design.” [arXiv](https://doi.org/10.48550/ARXIV.2311.04076)\n",
    "\n",
    "\n",
    "- Simulating Social Relationships\n",
    "\t-\tAkata, Elif, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. (2023). Playing Repeated Games with Large Language Models. [arXiv](https://doi.org/10.48550/ARXIV.2305.16867).\n",
    "\t-\tPark, Joon Sung, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. (2023). Generative Agents: Interactive Simulacra of Human Behavior. [arXiv](https://arxiv.org/abs/2304.03442).\n",
    "\t-\tWang, Lei, Chen Ma, Xueyang Zeng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, et al. (2024). A Survey on Large Language Model Based Autonomous Agents. Frontiers of Computer Science, 16(8), 186345. [link](https://doi.org/10.1007/s11704-024-40231-1).\n",
    "\n",
    "\n",
    "- Interacting with Human Agents\n",
    "\t-\tChopra, Felix, and Ingar Haaland. (2023). Conducting Qualitative Interviews with AI. SSRN Electronic Journal. [link](https://doi.org/10.2139/ssrn.4583756).\n",
    "\n",
    "\n",
    "- Text Annotation\n",
    "\t-\tGilardi, Fabrizio, Meysam Alizadeh, and Maël Kubli. (2023). ChatGPT Outperforms Crowd Workers for Text-Annotation Tasks. Proceedings of the National Academy of Sciences, 120(30), e2305016120. [link](https://doi.org/10.1073/pnas.2305016120).\t\n",
    "\t-\tLaurer, Moritz. (2024). Synthetic Data: Save Money, Time and Carbon with Open Source. Hugging Face Blog. [link](https://huggingface.co/blog/synthetic-data-save-costs).\n",
    "\t-\tLeek, Lauren Caroline, Simon Bischl, and Maximilian Freier. (2024). Introducing Textual Measures of Central Bank Policy-Linkages Using ChatGPT. [link](https://doi.org/10.31235/osf.io/78wnp).\n",
    "\t-\tTörnberg, Petter. (2023). ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. [arXiv](https://doi.org/10.48550/ARXIV.2304.06588).\n",
    "\n",
    "\n",
    "- Steamlining Research Pipeline\n",
    "\t-\tKorinek. (2023). Generative AI for Economic Research: Use Cases and Implications for Economists. Journal of Economic Literature, 61(4), 1281-1317. [link](https://www.aeaweb.org/articles?id=10.1257/jel.20231736)\n",
    "\n",
    "\n",
    "- Model Selection\n",
    "    -   Törnberg, Petter.(2024). Best Practices for Text Annotation with Large Language Models. Sociologica, 18(2).[link](https://sociologica.unibo.it/article/view/19461/18663)\n",
    "    -   Weber and Reichardt (2024). Evaluation is All You Need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer Using Open Models. [arXiv](https://arxiv.org/abs/2401.00284)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
